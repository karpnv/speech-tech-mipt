{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Nikonov, 2021-12-13 #"
      ],
      "metadata": {
        "id": "89SfBhAPOabv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLqUa76aAHIg",
        "outputId": "7596d41d-ad15-4f14-852f-0181c1a2aa7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox3\n",
            "Suggested packages:\n",
            "  file libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox3 sox\n",
            "0 upgraded, 8 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 760 kB of archives.\n",
            "After this operation, 6,717 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox3 amd64 14.4.2-3ubuntu0.18.04.1 [226 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2-3ubuntu0.18.04.1 [10.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-base amd64 14.4.2-3ubuntu0.18.04.1 [32.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 sox amd64 14.4.2-3ubuntu0.18.04.1 [101 kB]\n",
            "Fetched 760 kB in 1s (727 kB/s)\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../2-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../3-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../4-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../6-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../7-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt install sox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX_UvnL9FOB2"
      },
      "source": [
        "### Baseline commands recognition (2-5 points)\n",
        "\n",
        "We're now going to train a classifier to recognize voice. More specifically, we'll use the [Speech Commands Dataset] that contains around 30 different words with a few thousand voice records each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgdP8SO2xeot"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import display, Audio\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pickle\n",
        "import librosa\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from scipy.io import wavfile\n",
        "import IPython.display as ipd\n",
        "\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvHkw2rfY9k7",
        "outputId": "75bbe48d-f8e8-485d-ab8e-7541feeebc72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-10-24 14:31:22--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.13.128, 2a00:1450:400c:c03::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.13.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1489096277 (1.4G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.01.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   1.39G  88.0MB/s    in 18s     \n",
            "\n",
            "2021-10-24 14:31:41 (76.8 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n",
            "\n",
            "Classes: bed, bird, cat, dog, down, eight, five, four, go, happy, house, left, marvin, nine, no, off, on, one, right, seven, sheila, six, stop, three, tree, two, up, wow, yes, zero\n"
          ]
        }
      ],
      "source": [
        "datadir = \"speech_commands\"\n",
        "\n",
        "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
        "# alternative url: https://www.dropbox.com/s/j95n278g48bcbta/speech_commands_v0.01.tar.gz?dl=1\n",
        "!mkdir {datadir} && tar -C {datadir} -xvzf speech_commands_v0.01.tar.gz 1> log\n",
        "\n",
        "samples_by_target = {\n",
        "    cls: [os.path.join(datadir, cls, name) for name in os.listdir(\"./speech_commands/{}\".format(cls))]\n",
        "    for cls in os.listdir(datadir)\n",
        "    if os.path.isdir(os.path.join(datadir, cls))\n",
        "}\n",
        "print('Classes:', ', '.join(sorted(samples_by_target.keys())[1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME4cVShQ916w",
        "outputId": "7c3169dc-b7f1-4238-d550-374432f7bd78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input File     : 'speech_commands/bed/00176480_nohash_0.wav'\n",
            "Channels       : 1\n",
            "Sample Rate    : 16000\n",
            "Precision      : 16-bit\n",
            "Duration       : 00:00:01.00 = 16000 samples ~ 75 CDDA sectors\n",
            "File Size      : 32.0k\n",
            "Bit Rate       : 256k\n",
            "Sample Encoding: 16-bit Signed Integer PCM\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sox --info speech_commands/bed/00176480_nohash_0.wav"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WEPUzj1qN7F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF87vTlRs5qv"
      },
      "source": [
        "### Нарисуем спектрограмму \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBb8LEcRs35y"
      },
      "outputs": [],
      "source": [
        "def show_spectrogram(wav_path):\n",
        "\n",
        "    # Read the wav file (mono)\n",
        "    samplingFrequency, signalData = wavfile.read(wav_path)\n",
        "\n",
        "    plt.subplot(211)\n",
        "    plt.title('Spectrogram of a wav file')\n",
        "\n",
        "    plt.plot(signalData)\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Amplitude') \n",
        "\n",
        "    plt.subplot(212)\n",
        "    plt.specgram(signalData,Fs=samplingFrequency)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.show()\n",
        "    return ipd.Audio(wav_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTBuaYpktJVB"
      },
      "outputs": [],
      "source": [
        "show_spectrogram('/content/speech_commands/right/00b01445_nohash_0.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_LYOd_TtM6B"
      },
      "source": [
        "### Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvF5l-PCyd8z",
        "outputId": "e118fd1a-8dca-4fdc-9320-bbf7b4657c06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11834/11834 [07:32<00:00, 26.13it/s]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "import joblib as jl\n",
        "\n",
        "classes = (\"left\", \"right\", \"up\", \"down\", \"stop\")\n",
        "\n",
        "def preprocess_sample(filepath, max_length=150):\n",
        "    amplitudes, sr = librosa.core.load(filepath)\n",
        "    spectrogram = librosa.feature.melspectrogram(amplitudes, sr=sr)[:, :max_length]\n",
        "    spectrogram = np.pad(spectrogram, [[0, 0], [0, max(0, max_length - spectrogram.shape[1])]], mode='constant')\n",
        "    target = classes.index(filepath.split(os.sep)[-2])\n",
        "    return np.float32(spectrogram), np.int64(target)\n",
        "\n",
        "all_files = chain(*(samples_by_target[cls] for cls in classes))\n",
        "spectrograms_and_targets = jl.Parallel(n_jobs=-1)(tqdm(list(map(jl.delayed(preprocess_sample), all_files))))\n",
        "X, y = map(np.stack, zip(*spectrograms_and_targets))\n",
        "X = X.transpose([0, 2, 1])  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JSAwXkemEj4"
      },
      "source": [
        "### Модель с 2d-свертками"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHb3RCvSjxWt",
        "outputId": "2a5fd7d1-eb7f-4566-be86-4bb7736eaccd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11834, 1, 150, 128)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# X.shape\n",
        "X_r = X[:,None,:,:]\n",
        "X_r.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0BhnljvkZWN"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_r, y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ol6sywTG_Y9"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "tensor_x = torch.Tensor(X_train)\n",
        "tensor_y = torch.LongTensor(y_train)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "\n",
        "tensor_x = torch.Tensor(X_test) # transform to torch tensor\n",
        "tensor_y = torch.LongTensor(y_test)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                         shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                        shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC4IeBurqOYI"
      },
      "source": [
        "### Функция обучения модели\n",
        "__Параметры:__\n",
        "* модель\n",
        "* данные для обучения\n",
        "* количество эпох\n",
        "\n",
        "Функция возвращает лосс"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR1uxQ-GGGLr"
      },
      "outputs": [],
      "source": [
        "def model_train(model, trainloader, epochs):\n",
        "  total_step = len(trainloader)\n",
        "  losses, epoch_losses = [], []\n",
        "  for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      epoch_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()     \n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          epoch_loss += loss.item()\n",
        "          if (i+1) % 50 == 0:    # print every 50 mini-batches\n",
        "              print('Epoch [{}/{}], Step [{}/{}], loss: {:.3f}'\n",
        "                  .format(epoch, epochs, i+1, total_step, running_loss / 50))\n",
        "              running_loss = 0.0\n",
        "      epoch_losses.append(epoch_loss)\n",
        "\n",
        "\n",
        "  print('Finished Training')\n",
        "  return epoch_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sye_tt_jplnm"
      },
      "source": [
        "### Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qr8t6wCF8vT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net2d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: define your layers here\n",
        "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        # self.bn1 = nn.BatchNorm2D(16)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(4, 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.pool3 = nn.MaxPool2d(6, 2)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(64, 128, 5)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool4 = nn.MaxPool2d(6, 2)\n",
        "\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense1 = nn.Linear(256, 64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dense2 = nn.Linear(64, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: apply your layers here\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.pool4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dense2(x)\n",
        "        return F.softmax(x)\n",
        "\n",
        "\n",
        "net2d = Net2d().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net2d.parameters(), lr=0.003)#, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JQb7aZjqvoR"
      },
      "source": [
        "### Предсказания на тестовых данных\n",
        "\n",
        "Посмотрим accuracy в т.ч поклассово"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AEbZp1YxOJgc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UNsDwelBZVg"
      },
      "outputs": [],
      "source": [
        "def model_predict(model, testloader):\n",
        "  correct_pred = {classname: 0 for classname in classes}\n",
        "  total_pred = {classname: 0 for classname in classes}\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  # again no gradients needed\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          inputs, labels = data\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(inputs)\n",
        "          _, predictions = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predictions == labels).sum().item()\n",
        "          # collect the correct predictions for each class\n",
        "          for label, prediction in zip(labels, predictions):\n",
        "              if label == prediction:\n",
        "                  correct_pred[classes[label]] += 1\n",
        "              total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "  # print accuracy for each class\n",
        "  for classname, correct_count in correct_pred.items():\n",
        "      accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "      print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
        "                                                    accuracy))\n",
        "  print('Total accuracy: %d %%' % (\n",
        "      100 * correct / total))\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4HNvZYNsTja",
        "outputId": "42ac77bf-7cfb-4c7d-e86b-2a0cd9b983e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/20], Step [50/555], loss: 1.587\n",
            "Epoch [0/20], Step [100/555], loss: 1.569\n",
            "Epoch [0/20], Step [150/555], loss: 1.542\n",
            "Epoch [0/20], Step [200/555], loss: 1.531\n",
            "Epoch [0/20], Step [250/555], loss: 1.476\n",
            "Epoch [0/20], Step [300/555], loss: 1.477\n",
            "Epoch [0/20], Step [350/555], loss: 1.416\n",
            "Epoch [0/20], Step [400/555], loss: 1.452\n",
            "Epoch [0/20], Step [450/555], loss: 1.436\n",
            "Epoch [0/20], Step [500/555], loss: 1.388\n",
            "Epoch [0/20], Step [550/555], loss: 1.427\n",
            "Epoch [1/20], Step [50/555], loss: 1.402\n",
            "Epoch [1/20], Step [100/555], loss: 1.330\n",
            "Epoch [1/20], Step [150/555], loss: 1.382\n",
            "Epoch [1/20], Step [200/555], loss: 1.354\n",
            "Epoch [1/20], Step [250/555], loss: 1.320\n",
            "Epoch [1/20], Step [300/555], loss: 1.367\n",
            "Epoch [1/20], Step [350/555], loss: 1.314\n",
            "Epoch [1/20], Step [400/555], loss: 1.338\n",
            "Epoch [1/20], Step [450/555], loss: 1.357\n",
            "Epoch [1/20], Step [500/555], loss: 1.334\n",
            "Epoch [1/20], Step [550/555], loss: 1.307\n",
            "Epoch [2/20], Step [50/555], loss: 1.384\n",
            "Epoch [2/20], Step [100/555], loss: 1.274\n",
            "Epoch [2/20], Step [150/555], loss: 1.305\n",
            "Epoch [2/20], Step [200/555], loss: 1.275\n",
            "Epoch [2/20], Step [250/555], loss: 1.305\n",
            "Epoch [2/20], Step [300/555], loss: 1.317\n",
            "Epoch [2/20], Step [350/555], loss: 1.363\n",
            "Epoch [2/20], Step [400/555], loss: 1.246\n",
            "Epoch [2/20], Step [450/555], loss: 1.299\n",
            "Epoch [2/20], Step [500/555], loss: 1.292\n",
            "Epoch [2/20], Step [550/555], loss: 1.275\n",
            "Epoch [3/20], Step [50/555], loss: 1.252\n",
            "Epoch [3/20], Step [100/555], loss: 1.245\n",
            "Epoch [3/20], Step [150/555], loss: 1.236\n",
            "Epoch [3/20], Step [200/555], loss: 1.291\n",
            "Epoch [3/20], Step [250/555], loss: 1.222\n",
            "Epoch [3/20], Step [300/555], loss: 1.269\n",
            "Epoch [3/20], Step [350/555], loss: 1.321\n",
            "Epoch [3/20], Step [400/555], loss: 1.255\n",
            "Epoch [3/20], Step [450/555], loss: 1.235\n",
            "Epoch [3/20], Step [500/555], loss: 1.312\n",
            "Epoch [3/20], Step [550/555], loss: 1.276\n",
            "Epoch [4/20], Step [50/555], loss: 1.247\n",
            "Epoch [4/20], Step [100/555], loss: 1.259\n",
            "Epoch [4/20], Step [150/555], loss: 1.289\n",
            "Epoch [4/20], Step [200/555], loss: 1.221\n",
            "Epoch [4/20], Step [250/555], loss: 1.244\n",
            "Epoch [4/20], Step [300/555], loss: 1.253\n",
            "Epoch [4/20], Step [350/555], loss: 1.234\n",
            "Epoch [4/20], Step [400/555], loss: 1.224\n",
            "Epoch [4/20], Step [450/555], loss: 1.180\n",
            "Epoch [4/20], Step [500/555], loss: 1.260\n",
            "Epoch [4/20], Step [550/555], loss: 1.280\n",
            "Epoch [5/20], Step [50/555], loss: 1.224\n",
            "Epoch [5/20], Step [100/555], loss: 1.200\n",
            "Epoch [5/20], Step [150/555], loss: 1.330\n",
            "Epoch [5/20], Step [200/555], loss: 1.241\n",
            "Epoch [5/20], Step [250/555], loss: 1.242\n",
            "Epoch [5/20], Step [300/555], loss: 1.217\n",
            "Epoch [5/20], Step [350/555], loss: 1.174\n",
            "Epoch [5/20], Step [400/555], loss: 1.194\n",
            "Epoch [5/20], Step [450/555], loss: 1.201\n",
            "Epoch [5/20], Step [500/555], loss: 1.215\n",
            "Epoch [5/20], Step [550/555], loss: 1.202\n",
            "Epoch [6/20], Step [50/555], loss: 1.218\n",
            "Epoch [6/20], Step [100/555], loss: 1.236\n",
            "Epoch [6/20], Step [150/555], loss: 1.277\n",
            "Epoch [6/20], Step [200/555], loss: 1.266\n",
            "Epoch [6/20], Step [250/555], loss: 1.209\n",
            "Epoch [6/20], Step [300/555], loss: 1.211\n",
            "Epoch [6/20], Step [350/555], loss: 1.199\n",
            "Epoch [6/20], Step [400/555], loss: 1.157\n",
            "Epoch [6/20], Step [450/555], loss: 1.186\n",
            "Epoch [6/20], Step [500/555], loss: 1.247\n",
            "Epoch [6/20], Step [550/555], loss: 1.205\n",
            "Epoch [7/20], Step [50/555], loss: 1.209\n",
            "Epoch [7/20], Step [100/555], loss: 1.252\n",
            "Epoch [7/20], Step [150/555], loss: 1.260\n",
            "Epoch [7/20], Step [200/555], loss: 1.248\n",
            "Epoch [7/20], Step [250/555], loss: 1.246\n",
            "Epoch [7/20], Step [300/555], loss: 1.172\n",
            "Epoch [7/20], Step [350/555], loss: 1.177\n",
            "Epoch [7/20], Step [400/555], loss: 1.190\n",
            "Epoch [7/20], Step [450/555], loss: 1.146\n",
            "Epoch [7/20], Step [500/555], loss: 1.136\n",
            "Epoch [7/20], Step [550/555], loss: 1.145\n",
            "Epoch [8/20], Step [50/555], loss: 1.177\n",
            "Epoch [8/20], Step [100/555], loss: 1.236\n",
            "Epoch [8/20], Step [150/555], loss: 1.144\n",
            "Epoch [8/20], Step [200/555], loss: 1.287\n",
            "Epoch [8/20], Step [250/555], loss: 1.285\n",
            "Epoch [8/20], Step [300/555], loss: 1.231\n",
            "Epoch [8/20], Step [350/555], loss: 1.252\n",
            "Epoch [8/20], Step [400/555], loss: 1.203\n",
            "Epoch [8/20], Step [450/555], loss: 1.171\n",
            "Epoch [8/20], Step [500/555], loss: 1.201\n",
            "Epoch [8/20], Step [550/555], loss: 1.141\n",
            "Epoch [9/20], Step [50/555], loss: 1.151\n",
            "Epoch [9/20], Step [100/555], loss: 1.176\n",
            "Epoch [9/20], Step [150/555], loss: 1.215\n",
            "Epoch [9/20], Step [200/555], loss: 1.236\n",
            "Epoch [9/20], Step [250/555], loss: 1.227\n",
            "Epoch [9/20], Step [300/555], loss: 1.189\n",
            "Epoch [9/20], Step [350/555], loss: 1.184\n",
            "Epoch [9/20], Step [400/555], loss: 1.175\n",
            "Epoch [9/20], Step [450/555], loss: 1.206\n",
            "Epoch [9/20], Step [500/555], loss: 1.208\n",
            "Epoch [9/20], Step [550/555], loss: 1.200\n",
            "Epoch [10/20], Step [50/555], loss: 1.180\n",
            "Epoch [10/20], Step [100/555], loss: 1.205\n",
            "Epoch [10/20], Step [150/555], loss: 1.261\n",
            "Epoch [10/20], Step [200/555], loss: 1.193\n",
            "Epoch [10/20], Step [250/555], loss: 1.176\n",
            "Epoch [10/20], Step [300/555], loss: 1.160\n",
            "Epoch [10/20], Step [350/555], loss: 1.133\n",
            "Epoch [10/20], Step [400/555], loss: 1.148\n",
            "Epoch [10/20], Step [450/555], loss: 1.171\n",
            "Epoch [10/20], Step [500/555], loss: 1.191\n",
            "Epoch [10/20], Step [550/555], loss: 1.213\n",
            "Epoch [11/20], Step [50/555], loss: 1.135\n",
            "Epoch [11/20], Step [100/555], loss: 1.170\n",
            "Epoch [11/20], Step [150/555], loss: 1.147\n",
            "Epoch [11/20], Step [200/555], loss: 1.174\n",
            "Epoch [11/20], Step [250/555], loss: 1.195\n",
            "Epoch [11/20], Step [300/555], loss: 1.171\n",
            "Epoch [11/20], Step [350/555], loss: 1.185\n",
            "Epoch [11/20], Step [400/555], loss: 1.167\n",
            "Epoch [11/20], Step [450/555], loss: 1.196\n",
            "Epoch [11/20], Step [500/555], loss: 1.154\n",
            "Epoch [11/20], Step [550/555], loss: 1.293\n",
            "Epoch [12/20], Step [50/555], loss: 1.220\n",
            "Epoch [12/20], Step [100/555], loss: 1.256\n",
            "Epoch [12/20], Step [150/555], loss: 1.226\n",
            "Epoch [12/20], Step [200/555], loss: 1.179\n",
            "Epoch [12/20], Step [250/555], loss: 1.181\n",
            "Epoch [12/20], Step [300/555], loss: 1.192\n",
            "Epoch [12/20], Step [350/555], loss: 1.299\n",
            "Epoch [12/20], Step [400/555], loss: 1.270\n",
            "Epoch [12/20], Step [450/555], loss: 1.169\n",
            "Epoch [12/20], Step [500/555], loss: 1.169\n",
            "Epoch [12/20], Step [550/555], loss: 1.172\n",
            "Epoch [13/20], Step [50/555], loss: 1.149\n",
            "Epoch [13/20], Step [100/555], loss: 1.159\n",
            "Epoch [13/20], Step [150/555], loss: 1.249\n",
            "Epoch [13/20], Step [200/555], loss: 1.204\n",
            "Epoch [13/20], Step [250/555], loss: 1.160\n",
            "Epoch [13/20], Step [300/555], loss: 1.212\n",
            "Epoch [13/20], Step [350/555], loss: 1.192\n",
            "Epoch [13/20], Step [400/555], loss: 1.150\n",
            "Epoch [13/20], Step [450/555], loss: 1.142\n",
            "Epoch [13/20], Step [500/555], loss: 1.138\n",
            "Epoch [13/20], Step [550/555], loss: 1.136\n",
            "Epoch [14/20], Step [50/555], loss: 1.209\n",
            "Epoch [14/20], Step [100/555], loss: 1.191\n",
            "Epoch [14/20], Step [150/555], loss: 1.173\n",
            "Epoch [14/20], Step [200/555], loss: 1.215\n",
            "Epoch [14/20], Step [250/555], loss: 1.229\n",
            "Epoch [14/20], Step [300/555], loss: 1.232\n",
            "Epoch [14/20], Step [350/555], loss: 1.166\n",
            "Epoch [14/20], Step [400/555], loss: 1.185\n",
            "Epoch [14/20], Step [450/555], loss: 1.173\n",
            "Epoch [14/20], Step [500/555], loss: 1.171\n",
            "Epoch [14/20], Step [550/555], loss: 1.157\n",
            "Epoch [15/20], Step [50/555], loss: 1.180\n",
            "Epoch [15/20], Step [100/555], loss: 1.155\n",
            "Epoch [15/20], Step [150/555], loss: 1.151\n",
            "Epoch [15/20], Step [200/555], loss: 1.165\n",
            "Epoch [15/20], Step [250/555], loss: 1.142\n",
            "Epoch [15/20], Step [300/555], loss: 1.179\n",
            "Epoch [15/20], Step [350/555], loss: 1.194\n",
            "Epoch [15/20], Step [400/555], loss: 1.234\n",
            "Epoch [15/20], Step [450/555], loss: 1.190\n",
            "Epoch [15/20], Step [500/555], loss: 1.160\n",
            "Epoch [15/20], Step [550/555], loss: 1.253\n",
            "Epoch [16/20], Step [50/555], loss: 1.209\n",
            "Epoch [16/20], Step [100/555], loss: 1.144\n",
            "Epoch [16/20], Step [150/555], loss: 1.189\n",
            "Epoch [16/20], Step [200/555], loss: 1.164\n",
            "Epoch [16/20], Step [250/555], loss: 1.264\n",
            "Epoch [16/20], Step [300/555], loss: 1.267\n",
            "Epoch [16/20], Step [350/555], loss: 1.234\n",
            "Epoch [16/20], Step [400/555], loss: 1.186\n",
            "Epoch [16/20], Step [450/555], loss: 1.252\n",
            "Epoch [16/20], Step [500/555], loss: 1.175\n",
            "Epoch [16/20], Step [550/555], loss: 1.138\n",
            "Epoch [17/20], Step [50/555], loss: 1.176\n",
            "Epoch [17/20], Step [100/555], loss: 1.134\n",
            "Epoch [17/20], Step [150/555], loss: 1.142\n",
            "Epoch [17/20], Step [200/555], loss: 1.157\n",
            "Epoch [17/20], Step [250/555], loss: 1.272\n",
            "Epoch [17/20], Step [300/555], loss: 1.156\n",
            "Epoch [17/20], Step [350/555], loss: 1.192\n",
            "Epoch [17/20], Step [400/555], loss: 1.216\n",
            "Epoch [17/20], Step [450/555], loss: 1.191\n",
            "Epoch [17/20], Step [500/555], loss: 1.175\n",
            "Epoch [17/20], Step [550/555], loss: 1.174\n",
            "Epoch [18/20], Step [50/555], loss: 1.173\n",
            "Epoch [18/20], Step [100/555], loss: 1.195\n",
            "Epoch [18/20], Step [150/555], loss: 1.188\n",
            "Epoch [18/20], Step [200/555], loss: 1.137\n",
            "Epoch [18/20], Step [250/555], loss: 1.149\n",
            "Epoch [18/20], Step [300/555], loss: 1.136\n",
            "Epoch [18/20], Step [350/555], loss: 1.192\n",
            "Epoch [18/20], Step [400/555], loss: 1.232\n",
            "Epoch [18/20], Step [450/555], loss: 1.170\n",
            "Epoch [18/20], Step [500/555], loss: 1.220\n",
            "Epoch [18/20], Step [550/555], loss: 1.251\n",
            "Epoch [19/20], Step [50/555], loss: 1.144\n",
            "Epoch [19/20], Step [100/555], loss: 1.158\n",
            "Epoch [19/20], Step [150/555], loss: 1.194\n",
            "Epoch [19/20], Step [200/555], loss: 1.158\n",
            "Epoch [19/20], Step [250/555], loss: 1.122\n",
            "Epoch [19/20], Step [300/555], loss: 1.155\n",
            "Epoch [19/20], Step [350/555], loss: 1.192\n",
            "Epoch [19/20], Step [400/555], loss: 1.171\n",
            "Epoch [19/20], Step [450/555], loss: 1.218\n",
            "Epoch [19/20], Step [500/555], loss: 1.137\n",
            "Epoch [19/20], Step [550/555], loss: 1.139\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "epoch_losses = model_train(net2d, trainloader, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "r3EZ6aklsYlQ",
        "outputId": "0973cdf7-5c34-41b0-b6a7-f4b00e735e51"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(epoch_losses, '-')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.title('Losses')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwYQrrHtsd1r",
        "outputId": "48667873-983f-4bf6-c218-18a6dff0555d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for class left  is: 78.9 %\n",
            "Accuracy for class right is: 81.7 %\n",
            "Accuracy for class up    is: 78.6 %\n",
            "Accuracy for class down  is: 83.0 %\n",
            "Accuracy for class stop  is: 75.0 %\n",
            "Total accuracy: 79 %\n"
          ]
        }
      ],
      "source": [
        "model_predict(net2d, testloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "spotter_Nikonov.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}